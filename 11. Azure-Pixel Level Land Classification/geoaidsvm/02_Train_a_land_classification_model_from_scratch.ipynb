{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a land classification model from scratch\n",
    "\n",
    "In this notebook, you will train a neural network model to predict land use from aerial imagery using Microsoft's Cognitive Toolkit (CNTK). Later notebooks will illustrate how you can apply the trained model to new images, both in Jupyter notebooks and in ESRI's ArcGIS Pro.\n",
    "\n",
    "This tutorial will assume that you have already provisioned an NC series [Geo AI Data Science Virtual Machine](http://aka.ms/dsvm/GeoAI) and are using this Jupyter notebook while connected via remote desktop on that VM. If not, please see our guide to [provisioning and connecting to a Geo AI DSVM](https://github.com/Azure/pixel_level_land_classification/blob/master/geoaidsvm/setup.md).\n",
    "\n",
    "## Download supporting files\n",
    "\n",
    "The following command will use the [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy) utility to download sample data, a pre-trained model, and code to your VM. The file transfer may take a couple of minutes to complete. When finished, you should see a transfer summary indicating that all files were transferred successfully. (To begin executing this and other notebook cells, select it and press Ctrl+Enter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!AzCopy /Source:https://ai4ehackathons.blob.core.windows.net/landcovertutorial /SourceSAS:\"?se=2020-04-06T06%3A59%3A00Z&sp=rl&sv=2018-03-28&sr=c&sig=YD6mbqnmYTW%2Bs6guVndjQSQ8NUcV8F9HY%2BhPNWiulIo%3D\" /Dest:D:\\pixellevellandclassification /S\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can navigate to the `D:\\pixellevellandclassification` directory to examine the files we have transferred. In subdirectory `training_data` you will find that the sample data are composed of paired files of [National Agricultural Imagery Project](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) aerial images and land cover labels produced by the [Chesapeake Conservancy](http://chesapeakeconservancy.org/). While these data are stored in the common TIFF format, they are not readily viewable because they do not have the usual three (RGB) color channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python packages (should already be installed on Geo AI DSVM)\n",
    "\n",
    "The Python packages used by our code are pre-installed on the Geo AI Data Science VM. If running this tutorial on another machine, you may need to install a few less-common packages yourself:\n",
    "- `tifffile`: load and save TIFF images (available as a conda package on the conda-forge channel)\n",
    "- `cntk`: for training neural networks (Python wheels for Windows listed [here](https://docs.microsoft.com/cognitive-toolkit/setup-windows-python))\n",
    "- `gdal`: read specialized headers in our TIFF files that contain information on the region shown, geospatial coordinate system used, etc. (we recommend [Christoph Gohlke's Python wheel](https://www.lfd.uci.edu/~gohlke/pythonlibs/) for a fast install)\n",
    "- `pyproj`: to read PROJ.4-formatted geospatial projection information (available as a conda package in the default channel)\n",
    "- `basemap`: to help convert between lat-lon coordinates and row/column positions in our data files (available as a conda package on the conda-forge channel)\n",
    "\n",
    "## Perform training\n",
    "\n",
    "Before starting training, ensure that you do not have any running processes making use of GPUs. (This may be the case if you have other programs or Jupyter notebooks running.) To do so, execute the code cell below to check your GPU status and running processes using `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "proc = subprocess.Popen('nvidia-smi', stdout=subprocess.PIPE)\n",
    "print(proc.stdout.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive an error stating that \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver,\" you may be using an Azure VM with no NVIDIA GPU. Please use an NC series VM as recommended above.\n",
    "\n",
    "To run the training script, edit the command below by replacing `%num_gpus%` with the number of GPUs on your VM:\n",
    "\n",
    "Geo AI DSVM SKU name | Number of GPUs\n",
    ":----:|:----:\n",
    "NC6 | 1\n",
    "NC12 | 2\n",
    "NC24 | 4\n",
    "\n",
    "After editing the command appropriately, open a Windows command prompt (e.g. by clicking on the Start menu, typing \"Command Prompt\", and pressing Enter), paste in the command, and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpiexec -n %num_gpus% C:\\Anaconda\\python ^\n",
    "    D:\\pixellevellandclassification\\scripts\\train_distributed.py ^\n",
    "    --input_dir D:\\pixellevellandclassification\\training_data ^\n",
    "    --model_dir D:\\pixellevellandclassification\\models ^\n",
    "    --num_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script will generate a new model from scratch, train the model for one epoch, and save the model to `D:\\pixellevellandclassification\\models\\trained.model`. Training takes ~25 minutes with a single GPU, ~15 minutes with two GPUs, etc.\n",
    "\n",
    "During this time, you can finish reading this notebook and monitor progress as follows:\n",
    "- In the command prompt where you launched the training, you should soon see output messages indicating the number of GPUs (\"nodes\") participating.\n",
    "- Using Task Manager, observe that a Python process has been spawned for each GPU and is using a substantial amount of memory.\n",
    "    - This tutorial uses eight pairs of training files. They occupy more space in memory than they do on disk due to decompression on loading.\n",
    "    - Because it takes so long to load files of this size, we've chosen to load them once at the beginning of training and hold them in memory for fast access. This is especially beneficial when training for more than one epoch.\n",
    "- Re-run the `nvidia-smi` cell above: you should see utilization of all GPUs (eventually resulting in high temperature and GPU memory usage) and one running process per GPU.\n",
    "\n",
    "When training is complete, the output messages at the command prompt should indicate the duration of the training epoch and the error rate on the training set during the epoch, e.g.\n",
    "```\n",
    "Finished Epoch[1 of 1]: [Training] loss = 0.127706 * 16000, metric = 3.59% * 16000 1421.583s ( 11.3 samples/s);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the training script\n",
    "\n",
    "While training runs, take a moment to explore the training script and model definition in your favorite text editor:\n",
    "```\n",
    "D:\\pixellevellandclassification\\scripts\\train_distributed.py\n",
    "D:\\pixellevellandclassification\\scripts\\model_mini_pub.py\n",
    "```\n",
    "\n",
    "Below we provide some additional explanation of selected sections of these scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data access\n",
    "\n",
    "Near the beginning of the training script is a custom minibatch source specifying how the training data should be read and used. Our training data comprise pairs of TIF images. The first image in each pair is a four-channel (red, green, blue, near-infrared) aerial image of a region of the Chesapeake Bay watershed. The second image is a single-channel \"image\" corresponding to the same region, in which each pixel's value corresponds to a land cover label:\n",
    "- 0: Unknown land type\n",
    "- 1: Water\n",
    "- 2: Trees and shrubs\n",
    "- 3: Herbaceous vegetation\n",
    "- 4+: Barren and impervious (roads, buildings, etc.); we lump these labels together\n",
    "\n",
    "These two images in each pair correspond to the features and labels of the data, respectively. The minibatch source specifies that the available image pairs should be partitioned evenly between the workers, and each worker should load its set of image pairs into memory at the beginning of training. This ensures that the slow process of reading the input images is performed only once per training job. To produce each minibatch, subregions of a given image pair are sampled randomly. Training proceeds by cycling through the image pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model architecture\n",
    "The [model definition script](https://aiforearthcollateral.blob.core.windows.net/imagesegmentationtutorial/scripts/model_mini_pub.py) specifies the model architecture: a form of [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The input for this model will be a 256 pixel x 256 pixel four-channel aerial image (corresponding to a 256 meter x 256 meter region), and the output will be predicted land cover labels for the 128 m x 128 m region at the center of the input region. (Predictions are not provided at the boundaries due to edge effects.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you have produced a trained model, you can test its performance in the following notebook on [applying your model to new aerial images](./03_Apply_trained_model_to_new_data.ipynb). You may later wish to return to this section to:\n",
    "- Train a model for more than one epoch to improve its performance\n",
    "- Train with fewer GPUs to confirm the runtime scaling achieved with distributed training (NC12 and NC24 VMs only)\n",
    "\n",
    "When you are done using your Geo AI Data Science VM, we recommend that you stop or delete it to prevent further charges.\n",
    "\n",
    "For comments and suggestions regarding this notebook, please post a [Git issue](https://github.com/Azure/pixel_level_land_classification/issues/new) or submit a pull request in the [pixel-level land classification repository](https://github.com/Azure/pixel_level_land_classification)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
