{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a config file with paths to all the raw data. Each entry of the resulting yaml file specifies the source satellite image and the shapefiles over which to create masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/datadrive/glaciers/\")\n",
    "\n",
    "# label shape files\n",
    "glaciers_file = data_dir / \"vector_data/Glacier_2005.shp\"\n",
    "clean_g_file = data_dir / \"vector_data/clean.shp\"\n",
    "debris_g_file =  data_dir / \"vector_data/debris.shp\"\n",
    "border_file = data_dir / \"vector_data/hkh.shp\"\n",
    "\n",
    "# directories for configuration and output\n",
    "masking_file = \"../conf/masking_paths.yaml\"\n",
    "process_file = \"../conf/postprocess.yaml\"\n",
    "process_dir = data_dir / \"processed\"\n",
    "output_dir = process_dir / \"patches\"\n",
    "input_folder = data_dir / \"unique_tiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET = False # set to False to run on full data\n",
    "\n",
    "paths = {}\n",
    "input_paths = list(Path(input_folder).glob(\"*.tif*\"))\n",
    "if SUBSET is not False:\n",
    "    input_paths = input_paths[:SUBSET]\n",
    "\n",
    "for i, f in enumerate(input_paths):\n",
    "    mask_ele = {}\n",
    "    mask_ele[\"img_path\"] = str(f)\n",
    "    mask_ele[\"mask_paths\"] = [str(s) for s in [glaciers_file, clean_g_file, debris_g_file]]\n",
    "    mask_ele[\"border_path\"] = str(border_file )\n",
    "    paths[f\"mask_{i}\"] = mask_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(masking_file, 'w') as f:\n",
    "    yaml.dump(paths, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the configuration file `masking_paths.yaml`, we can create numpy masks that are aligned with the underlying numpy images. You can see from the plot below that the four channels of the mask correspond to (1) all the glaciers, (2) the clean ice glaciers, (3) the debris-covered glaciers, and (4) the country masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.mask import generate_masks\n",
    "import shutil\n",
    "\n",
    "masking_paths = yaml.safe_load(open(masking_file))\n",
    "img_paths = [p[\"img_path\"] for p in masking_paths.values()]\n",
    "mask_paths = [p[\"mask_paths\"] for p in masking_paths.values()]\n",
    "border_paths = [p[\"border_path\"] for p in masking_paths.values()]\n",
    "mask_dir = process_dir / \"masks\"\n",
    "\n",
    "if mask_dir.exists():\n",
    "    shutil.rmtree(mask_dir)\n",
    "    \n",
    "generate_masks(img_paths, mask_paths, border_paths=border_paths, out_dir=mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "mask = np.load(mask_dir / \"mask_01.npy\")\n",
    "border = np.load(mask_dir / \"border_01.npy\")\n",
    "_, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "ax[0].imshow(mask[:, :, 0])\n",
    "ax[1].imshow(mask[:, :, 1])\n",
    "ax[2].imshow(mask[:, :, 2])\n",
    "ax[3].imshow(border)\n",
    "\n",
    "mask_df = pd.read_csv(mask_dir / \"mask_metadata.csv\")\n",
    "mask_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have binary masks associated with each image, we can slice them into 512 x 512 patches to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.slice import write_pair_slices\n",
    "\n",
    "paths = pd.read_csv(mask_dir / \"mask_metadata.csv\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = []\n",
    "for row in range(len(paths)):\n",
    "    print(f\"## Slicing tiff {row +1}/{len(paths)} ...\")\n",
    "    metadata_ = write_pair_slices(\n",
    "        paths.iloc[row][\"img\"],\n",
    "        paths.iloc[row][\"mask\"],\n",
    "        output_dir,\n",
    "        border_path=paths.iloc[row][\"border\"],\n",
    "        out_base=f\"patch_{paths.index[row]}\"\n",
    "    )\n",
    "    metadata.append(metadata_)\n",
    "\n",
    "metadata = pd.concat(metadata, axis=0)\n",
    "out_path = Path(output_dir, \"patches.geojson\")\n",
    "metadata.to_file(out_path, index=False, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.slice import plot_slices\n",
    "plot_slices(output_dir, n_cols=4, div=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sliced each tiff into small patches, we can determine which to use for training, validation, and testing. We first filter away those patches that have relatively little glacier, then we randomly shuffle them into train, dev, and test directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "import geopandas as gpd\n",
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "import yaml\n",
    "\n",
    "pconf = Dict(yaml.safe_load(open(process_file, \"r\")))\n",
    "slice_meta = gpd.read_file(output_dir / \"patches.geojson\")\n",
    "\n",
    "# filter all the slices to the ones that matter\n",
    "print(\"filtering\")\n",
    "keep_ids = pf.filter_directory(\n",
    "    slice_meta,\n",
    "    filter_perc=pconf.filter_percentage,\n",
    "    filter_channel=pconf.filter_channel,\n",
    ")\n",
    "\n",
    "# validation: get ids for the ones that will be training vs. testing.\n",
    "print(\"reshuffling\")\n",
    "split_method = [item for item in pconf.split_method.items()][0][0]\n",
    "split_ratio = pconf.split_method[split_method].split_ratio\n",
    "split_fun = getattr(pf, split_method)\n",
    "split_ids = split_fun(keep_ids, split_ratio, slice_meta=slice_meta)\n",
    "target_locs = pf.reshuffle(split_ids, process_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, it's useful to save which patches went into which split. These will be contained in the `target_locks.pickle` file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "target_locs_file = process_dir / \"target_locs.pickle\"\n",
    "with open(target_locs_file, \"wb\") as f:\n",
    "    pickle.dump(target_locs, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to postprocess the patches that have been selected for training. We normalize all the input channels and add a \"background\" output channel, to support multiclass classification. Next, we perform the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "\n",
    "print(\"Normalizing input channels.\")\n",
    "stats = pf.generate_stats(\n",
    "    [p[\"img\"] for p in target_locs[\"train\"]],\n",
    "    pconf.normalization_sample_size,\n",
    "    pconf.process_funs.normalize.stats_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now complete any other postprocessing specified by the processing configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_type in target_locs:\n",
    "    for k in range(len(target_locs[split_type])):\n",
    "        img, mask = pf.postprocess(\n",
    "            target_locs[split_type][k][\"img\"],\n",
    "            target_locs[split_type][k][\"mask\"],\n",
    "            pconf.process_funs,\n",
    "        )\n",
    "        \n",
    "        np.save(target_locs[split_type][k][\"img\"], img)\n",
    "        np.save(target_locs[split_type][k][\"mask\"], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaciers",
   "language": "python",
   "name": "glaciers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
